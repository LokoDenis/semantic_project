# ReadMe
## Semantic_project
(Sooner or later this would be partly translated into English. Also, there would be part with description of added folders and files.)
### Задача проекта
Семантическая сегментация изображений --- это разделение изображений на группы пикселей, отвечающих разным типам объектов. Задача имеет практическую важность, например, в картографии, позволяя упростить процесс разметки карт. На данный момент существуют различные решения этой задачи, в основном основанные на применении различных подходов, основанных либо на машинном обучении, либо на сверточных нейронных сетях.

### План работы

1) Написание утилиты, оценивающей качество разметки изображения.
Так как задачи публикуются в общей системе http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html, неплохо было бы иметь свою утилитку--тулзу для оценки качества проделанной работы. Это позволит в реальном времени видеть, какие усовершенствования в программе ведут к улучшению результата, а какик --- нет.

2) Знакомство с основными методами сегментации изображения.
Можно рассмотреть следующие виды сегментации изображений:Meanshift/Quickshift, Efficient Graph-based, Turbopixel, SLIC. Идея в том, чтобы в дальнейшем работать не с отдельными пикселями, а с уже сгруппированными суперпикселями. Но для этого нужно быть уверенными в том, что каждый суперпиксель содержит в себе только точки одного класса разбиения. 

3) Выбор параметров для машинного обучения.
Чтобы обучаться, нужно выделить много характерных признаков. Для этого мы вспоминаем о таких вещах, как средние яркости, их дисперсии, различного рода ковариационные матрицы. Так как этого еще явно мало, сюда же добавляем возможность поэкспериментировать с линейными фильтрами и разностными признаками (предпочтительно Haar-like). Также можно работать и учитывая соседство пикселей (суперпикселей), для этого потребуется знание разных видов дескрипторов и методов кластеризации. В связке с подходом типа Bag of Words можно уменьшить размерность задачи, сделав выделяемые признаки более глобальными. Это поможет в ситуации, когда данных для обучения не так много.

4) Обучение.
Когда какие-то признакми уже выделены, можно попробовать пообучаться на классификаторе Random Forest или его аналогах.

5) Оценка обучения.
Вполне понятно, что нужно уметь оценивать качество самого обучения из рассчета того, насколько хорошо построенная модель будет работать на независимых данных. Для этого нужно ознакомиться с такими методами, хотя бы с кросс-вадидацией и пр.

6) Постобработка.
После обучения картинка все еще не имеет финальный вид, ведь возможны какие-то погрешности разбиения, смазанные границы, выбросы, например. Для "сглаживания углов" применяются несколько методов. Можно попробовать использовать здесь методы, основанные на Conditional Random Field (CRF) и минимизации энергии унарного и парного потенциалов. Возможных реализаций здесь очень много, в том числе можно использовать такие общие методы как Graph-cut, или же попробовать прикрутить сюда же уже отчасти знакомый SGM. Это интересено.

7) Ознакомление со статьями других людей.
Изучение данной темы на данный момент идет полным ходом. Поэтому имеет смысл ознакомиться хотя бы с десятком подобных работ, чтобы представлять, какие подходы на данный момент используются чаще всего и почему, что уже доказало свою эффективность или неэффективность. На основе таких данных определенно появятся дополнительные идеи, как можно улучшить свою реализацию. 

### Who am I?
Работа ведется студентом второго курса ПМИ ФКН ВШЭ Беляковым Денисом под руководством к.ф-м.н. Горбачева Вадима. 

